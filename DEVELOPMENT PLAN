MarketPeak MVP Development Plan
This plan outlines the tasks required to build the MarketPeak MVP across the Frontend (lovable.dev) and Backend (Cursor). The focus is on core MVP features, with any extra features clearly marked as Post-MVP to be tackled later. For consistency, all references to "organization" from the original spec are replaced with "business" (e.g., tables, fields, and UI labels use "business").
Frontend (lovable.dev) – MVP Tasks
The frontend will be built with React (Vite) and styled using Tailwind CSS with shadcn/ui components. We will use TanStack Query for data fetching and caching. The tasks below are grouped by feature area, in a logical implementation order:
1. Project Setup and Dependencies
Initialize React Project: Set up a new React project using Vite (with TypeScript for type safety). Ensure the development server runs correctly.
Install Tailwind CSS: Add Tailwind CSS to the project. Configure Tailwind (create tailwind.config.js, import Tailwind directives in CSS) for styling utility classes. Verify Tailwind is working by styling a test element.
Integrate shadcn/ui Components: Install shadcn/ui (Radix UI + Tailwind components). Import a basic component (e.g. Button or Input) to verify the styling matches the design system. This will provide pre-built accessible UI components (modals, dropdowns, etc.) for faster development.
Set up Routing: Install React Router (or an equivalent routing library) to manage pages. Create basic routes for Login, Signup, Dashboard, etc., even if the pages are initially placeholders. This allows navigation structure early on.
Configure TanStack Query: Install @tanstack/react-query. Wrap the app in a QueryClientProvider with a created QueryClient. This will be used for data fetching (e.g., Supabase queries) with caching and loading states handled easily.
Environment Configuration: Create an .env file to store environment variables like Supabase URL and anon API key (for client), plus any other config (e.g., feature flags). Load these in the Vite config and ensure they are accessible in the app (e.g., via import.meta.env).
2. Authentication Pages & Flow (Supabase Auth)
Supabase Client Setup: Install Supabase JS client (@supabase/supabase-js) in the frontend. Initialize a Supabase client instance with the project URL and anon public key. This will be used for authentication and data operations on the client side.
Login Page UI: Create a Login page with a form for email and password. Use shadcn/ui components (e.g., <Input> and <Button>) for styling. Keep the layout simple and responsive (Tailwind for spacing and alignment).
Include a "Login" button and a link to the Signup page (“Don’t have an account? Sign Up”).
If applicable, include a "Forgot Password" link (MVP can optionally handle password reset via Supabase).
Signup Page UI: Create a Signup page with a form to collect email, password, and (optional) the user’s name. Use shadcn/ui form components. The form should have a "Sign Up" button and a link back to Login.
Ensure the page clearly states that a new account will be created and possibly a business can be created after sign-up (or in the same flow – see Business creation tasks).
Supabase Auth Integration: For both Login and Signup forms, implement form submission handlers:
On Signup, call supabase.auth.signUp({ email, password }). After successful sign-up, you may need to insert a profile or redirect to a business creation flow (depending on backend logic). Handle errors (e.g., email already in use) and show friendly error messages.
On Login, call supabase.auth.signInWithPassword({ email, password }). If successful, store the returned session/user (Supabase client does this internally and provides supabase.auth.getUser() for current user). On success, redirect the user to the Dashboard. Handle login errors (invalid credentials) with an error message.
Auth State & Routing Protection: Implement a way to track auth state on the client:
Use Supabase’s auth state change listener (e.g., supabase.auth.onAuthStateChange) to respond to login/logout automatically or use React state to store the current user.
Set up protected routes for all pages after login (Dashboard, Profile, etc.). For example, if no current user, redirect to Login page. Conversely, if user is logged in, they shouldn’t see Login/Signup.
Logout Functionality: Add a logout button (for example, in the Dashboard navbar or profile menu). On click, call supabase.auth.signOut() to log the user out and redirect back to the Login page. Ensure session is cleared.
3. Business Creation & Membership UI
Business Context Setup: After logging in (especially immediately after sign-up), if the user is not yet part of a business, they should be prompted to create or join one. Create a page or modal for Business Selection/Creation that appears if the user has no active business.
Create Business Form: Design a form (modal or dedicated page) to Create a Business (this replaces the old “organization” creation). Fields should include at least a Business Name (and maybe an optional business avatar or description). Use shadcn/ui input components for consistency.
On submit, call the backend (Supabase function or RPC) or use Supabase client to insert a new business record into the database. For MVP, simply creating the business with the current user as owner is enough.
After successful creation, update the UI to set this business as the current active business and proceed to the Dashboard.
Handle errors (e.g., business name already taken, though unlikely needed to enforce uniqueness globally) and show feedback (success or error toast).
Join Business Flow: If the user is intended to join an existing business (e.g., via invitation):
Provide an input for an invite code or link (MVP could be a simple text code that another user provides). On submit, call backend to validate and add the user to that business’s members.
Alternatively, list pending invitations for the user (if backend stores invites tied to email) and let them accept. Note: Invitation system can be a Post-MVP feature if it complicates MVP; for now, you might skip this unless easy to implement.
If joining succeeds, set that business as active and go to Dashboard.
Ensure UI labels here also use "business" (e.g., “Join a Business”).
Business Switcher: In the Dashboard’s header or sidebar, include a Business Switcher UI (especially if users can belong to multiple businesses). This can be a dropdown listing the names of businesses the user is a member of:
On selecting a different business, update the active business context in the app (e.g., store active business ID in React state or context).
Ensure subsequent data fetching (content, etc.) uses the active business ID.
If only one business per user for MVP, this can be simplified (or omitted), but structure the UI to accommodate multiple in the future.
Rename Labels to "Business": Do a pass through all UI text to confirm that any mention of “organization” is replaced with “business”. For example, buttons should say “Create Business”, dashboard header “Business Name” etc., reflecting the new terminology.
4. Dashboard Layout & Navigation
Dashboard Page Skeleton: Create a Dashboard page that will serve as the main app screen once the user is logged in and has a business selected. Implement a basic layout:
A top navigation bar or header that shows the app name (MarketPeak), possibly the current business name, and icons/links to Profile, Settings, or Logout.
A sidebar or menu (especially on desktop) listing main sections: e.g., Content, Inbox, Analytics, Settings (some of these might be placeholders for now).
The central area which will display content based on the selected section (default could be a welcome message or the Content creation UI).
Responsive Design: Use Tailwind utility classes to ensure the dashboard layout is responsive:
On smaller screens, the sidebar may collapse into a hamburger menu. Ensure the navigation can toggle on mobile.
Test the layout for different screen sizes to make the UI mobile-friendly (important for a good MVP impression).
Navigation Links: Implement React Router links or <NavLink> for the main sections. For MVP, implement routing for at least:
Content Creation/Management (the main dashboard view for creating AI content).
Profile/Account (profile settings page).
(If implementing now) Analytics and Inbox can be links that either lead to placeholders or are hidden if Post-MVP.
Default Dashboard Content: On the main dashboard route, show a simple welcome section if no specific feature is selected:
Greet the user by name or show the business name and a welcome message.
Include a quick shortcut button to start creating content (link to the Content creation UI section).
This ensures the user has guidance on what to do first (e.g., “Generate your first marketing content using AI!”).
Loading and Error States: Using TanStack Query and Supabase, ensure that as data loads (like fetching existing content or profile info), appropriate loading spinners or skeletons (perhaps using shadcn/ui skeleton component) are shown. If there are errors (e.g., failed to fetch data), display a friendly error message or toast so the user isn’t stuck without feedback.
5. User Profile & Settings (Frontend)
Profile Page: Create a Profile page where users can view and edit their personal information. This might include:
Displaying the user’s name, email (email is from Supabase Auth, likely uneditable for now except via a separate process), and maybe an avatar.
An edit form to update profile fields like name or avatar. (Avatar upload could be Post-MVP if using file storage, or use Gravatar or simple initials as placeholder.)
Use shadcn/ui form components for consistency. For file upload (avatar), integrate with a file input and preview if implemented.
Fetch & Display Profile Data: On page load, use TanStack Query to fetch the current user's profile data from the database (e.g., from a profiles table or Supabase auth metadata):
Create a query hook (e.g., useQuery(['profile', userId], ...) that fetches profile by user ID).
Display the retrieved info in read-only form fields or text, with an “Edit” button to toggle into edit mode (or navigate to a separate edit screen).
Update Profile Data: Implement functionality to update the profile:
If a separate Edit Profile form is used (or inline editing), allow the user to change their display name and upload a new avatar.
On save, call Supabase (either directly supabase.from('profiles').update(...) or via a backend endpoint) to persist changes. Use TanStack Query’s useMutation for the update and invalidate the profile query to refresh the data.
Show a success message (toast or alert) on successful save, or an error message if something fails.
Change Password (Optional for MVP): Optionally, provide a way for the user to change their password or handle password reset:
Supabase can send password reset emails via supabase.auth.resetPasswordForEmail(email) if configured. For MVP, you can simply include a "Reset Password" link that triggers an email to the logged-in user’s email (this requires enabling the SMTP settings in Supabase). This might be considered Post-MVP if time is short.
If implemented, confirm to the user that an email was sent.
Business Settings (if applicable): If the user is a business owner, provide a section in the profile or a separate Business Settings page accessible from the Dashboard:
Display the current business’s details (name, maybe business avatar/logo).
Allow editing the business name or other settings (persist via backend).
Possibly list team members (if multiple users per business is enabled in MVP) – if so, allow owner to remove members or see their roles. (Member management could be a Post-MVP enhancement if not critical now.)
Ensure any such UI also uses the term “Business” instead of “Organization”.
6. AI Content Generation Interface
Content Creation Page: Create a dedicated Content page in the dashboard for AI-generated content. This is a core feature of MarketPeak. Layout for this page could include:
A text input area or form where the user can enter a prompt or select parameters for content generation (e.g., a prompt textarea, and maybe dropdowns for tone or content type like "Blog Post", "Social Media Caption", etc.).
A "Generate Content" button to submit the request.
An output area where the generated content (text and/or images) will be displayed.
Prompt Form Design: Use shadcn/ui components to design an intuitive form:
Include a large Textarea for the user prompt or a set of fields if using a structured prompt (e.g., "Topic", "Keywords", etc., depending on desired UX).
Possibly include an option to choose the format of output (short form, long form, tweet, ad copy, etc.) – this can just affect the prompt structure internally.
If image generation is part of the flow, provide a checkbox or toggle like “Include an image” or a separate button to generate an image after text is generated.
Calling the Backend API: When the user submits the generate form, trigger an API call to the backend (Cursor) to process the AI content generation:
Use fetch or Axios to call the backend endpoint (e.g., POST /api/generate-content) with necessary data: prompt text, selected options, and the current business ID (so the content can be associated with the correct business on the backend).
Before calling, provide immediate feedback on the UI, such as disabling the form and showing a spinner or "Generating..." message.
Use TanStack Query’s useMutation for this API call to manage loading and error states easily.
Display Generated Content: Once the API responds with generated content:
Display the result in the output section. For text, show it in a formatted way (maybe inside a card or a read-only text area for easy copy). For images (if any), render the image in an <img> tag or a styled component.
If the backend responds immediately with content, show it directly. If the backend uses a job queue and returns a job ID or indicates processing, handle that (maybe poll the backend or use WebSocket – see below).
Consider using a toast notification for success (content ready) and error (generation failed) cases, in addition to updating the UI area.
Progress Updates (if asynchronous): If content generation might take a while (especially with GPT-4 or image generation), implement a strategy to inform the user:
Simplest approach: keep the button disabled and show a loading state until the response comes back (this is fine if using a direct call that waits for OpenAI).
Advanced (Post-MVP optional): use a WebSocket or Supabase Realtime channel to get progress or completion events. Alternatively, poll an endpoint with the job ID to see if it's done. For MVP, a direct wait is acceptable given OpenAI responses will be a few seconds.
Saving Generated Content: Once content is generated, provide an option to save it to the database (if the backend isn’t already doing so automatically):
You could have the backend save the content and return an ID. If not, on the frontend, you can offer a "Save Content" button that calls Supabase to store the text (and image URL if any) in a content table, associated with the current business and user.
Upon saving, perhaps show a confirmation like “Content saved!” and possibly list it in a content library.
Content History List (Optional MVP): It’s helpful for users to see previously generated content:
Below the generation form, show a list of saved content items (title or first line of text, timestamp). Use TanStack Query to fetch content entries for the current business (e.g., supabase.from('content').select('*') filtered by business_id).
Each item could have actions like “View” (to open or expand the full content), or “Delete” if they want to remove it.
Implement basic UI to view a content item (could be a modal that shows the full text and any image, with an option to copy the text).
This feature can be minimal at first (even just a list of texts). If time is short, content might be generated and not stored persistently (but persistence is typically expected, so it’s good to include).
Error Handling: If the AI generation fails (e.g., network error or API error):
Ensure the user sees an error message. The backend should return an error status and message which you can display near the form or as a toast (“Failed to generate content, please try again”).
Re-enable the form so they can modify prompt and retry.
7. Content Distribution Interface
Note: Basic distribution of generated content (e.g., sharing or publishing it) is part of MVP, but full multi-channel scheduling might be limited. We’ll include minimal functionality here and plan advanced features in Post-MVP.
Publish/Share Options: After content is generated (and maybe saved), provide UI controls to distribute the content. This can be a simple section on the content detail or generation page:
A “Publish” button to instantly share the content to a default channel (for example, post to Twitter or another integrated platform).
Alternatively, a “Copy to Clipboard” button so the user can manually post it (this is a quick MVP fallback if direct integration isn’t ready).
If multiple channels are supported in MVP (e.g., Twitter and LinkedIn), you could show a dropdown or list of channels with checkboxes to choose where to post.
Account Connection (Simplified for MVP): If publishing directly, the user needs to have connected their social account:
For MVP, you might hard-code a scenario or use developer credentials. (Full OAuth flows to connect user accounts to Twitter/LinkedIn could be Post-MVP due to complexity.)
Alternatively, allow the user to input API keys or access tokens for their accounts in a settings page (not ideal for security, but a possible interim solution for MVP testing).
If skipping real OAuth, clearly label this as a temporary method for MVP or use your own account keys for testing.
Schedule Post (Optional): If time permits, provide a field to schedule the posting time:
Could be a datetime picker (shadcn/ui has a date/time picker component). If the user selects a future time, the frontend will send this to backend to schedule via the job queue.
If no time is selected, assume immediate posting.
For MVP simplicity, scheduling can be skipped or limited to immediate posting.
Trigger Distribution: On clicking “Publish” (or “Schedule”), call a backend API endpoint (e.g., POST /api/distribute) with details: content ID (or the content text itself), target channel(s), and optional scheduled time.
Show feedback: if scheduled, indicate “Post scheduled for X”; if immediate, indicate “Posting...”.
The backend will handle the actual posting via job queue (details in backend tasks). The frontend should optimistically maybe mark the content as "publishing" or just wait for confirmation.
Confirmation and Status: After the backend confirms the content was posted (or queued):
If immediate, the API can return success once posted. Show a success message like “Content published to Twitter! ✅”.
If scheduled, the API might return that it was scheduled. Show “Content scheduled for [time].”
You might not see the result immediately, but for MVP, trust the backend. Optionally, list scheduled posts in an “Outbox” section with their times.
UI for Published Content (Optional): If the app is aware of published content details (like a URL of a Tweet):
Display the link or reference so the user can click to view it on the platform. For example, “View on Twitter” if a tweet URL was returned.
Store these details in the content record or a separate table so they can be shown in an Analytics section later.
8. Telemetry & Analytics (Frontend)
Event Logging Hooks: Integrate simple telemetry on the frontend to track user actions and app performance:
For critical actions like content generation, content publish, or errors, use the Supabase client or an API call to record an event. For example, after a user generates content, call supabase.from('events').insert({ type: 'generate', business_id, user_id, ... }) (the backend will have the events table).
Likewise, log events for “content_published”, “login”, “signup”, etc. This helps build internal analytics of feature usage.
This can often be done in the same code that handles the action success (e.g., after receiving generation result, fire off an async event log).
Performance Metrics on UI: Use the data from telemetry to show basic usage stats on the frontend if needed:
For MVP, this might not be user-facing, but you could display a simple Admin dashboard or console log to verify events are recording.
If there's an Analytics page for the user (business owner), you might show number of posts created, number of AI generations this week, etc. (This is likely Post-MVP to do nicely, but keep the thought for future.)
Unified Inbox UI (Post-MVP Placeholder): In the navigation, include a menu item or placeholder for Inbox (even if it’s not functional yet) to set expectations for the feature:
It can be a disabled link or a coming-soon page. For instance, clicking "Inbox" could show a message, “Unified Inbox is under development. Soon you'll see customer responses from all channels here.”
Using a placeholder ensures the menu layout won’t drastically change later and keeps the design consistent.
This item should be visibly separate or labeled as beta/coming-soon so users know it's not active.
Frontend tasks focus on delivering a functional UI and connecting it to backend APIs. Next, we outline the backend tasks required to power this frontend.
Backend (Cursor) – MVP Tasks
The backend consists of a Node.js/Express server (nicknamed "Cursor") and uses Supabase (PostgreSQL) for the database, authentication, and storage. It also integrates with external APIs like OpenAI for AI content and will use BullMQ with Redis for job queue processing. Key focus areas: setting up the database (with “business” multi-tenancy), implementing authentication logic, building the AI generation services, and enabling content distribution. All tasks referring to “organizations” will use the term “business” in schema and logic.
1. Supabase Project & Database Schema Setup
Supabase Project Initialization: If not already done, create a new Supabase project. Retrieve the project URL, anon key (for the frontend), and service role key (for backend admin actions if needed).
Configure Auth Settings: In Supabase Dashboard, enable Email/Password authentication provider. (Optionally enable email confirmations if desired, for MVP you might disable double opt-in to streamline signups). Set up SMTP if planning to use password recovery (can be skipped for MVP).
Create Tables – Business and Membership: Define the core tables using SQL or Supabase Studio:
businesses (replacing an “organizations” table): fields might include id (UUID or bigserial), name, created_at, and perhaps owner_id (references auth.users UID of the user who created it). Add any other relevant fields (e.g., description, logo URL).
business_members (membership linking users to businesses): fields: id (PK), business_id (FK to businesses), user_id (FK to auth.users or profiles), role (e.g., 'owner', 'member'), and created_at. Ensure a unique constraint on (business_id, user_id) to avoid duplicates. This table allows multi-user businesses and role management.
(Optional for MVP) profiles: Supabase often uses a public profiles table to store user info. Create a profiles table with id (PK, UUID, matches auth user ID), name, avatar_url, business_current (maybe store the last active business ID for convenience), and any profile info. This is useful to easily query user name, etc., since auth.users only has email/uid.
Create Tables – Content and Posts: Set up tables for AI content and distribution results:
content: to store generated content entries. Fields: id (PK), business_id (FK to businesses), user_id (who created it, FK to auth.users), title (optional short title or description), text (the main generated text), image_url (if an image was generated, store link to image in Supabase storage or external), created_at, published (boolean or status), published_at (timestamp if it was posted), platform (if published, which platform e.g. 'twitter'), platform_post_id (to store external ID or URL).
events: to log telemetry events. Fields: id (PK), business_id, user_id, event_type (e.g., 'login', 'content_generated', 'content_published'), data (JSONB field for any extra info, like prompt or error message), created_at. This will capture usage and help build analytics.
(Optional) invites: if implementing invitations for joining businesses, a table with id, business_id, invite_code or token, email (optional if tied to an email), created_by (user id of inviter), created_at, redeemed_by (user id of who joined) and redeemed_at. Invites can be Post-MVP if not doing now.
(Optional) messages or inbox: for unified inbox, a table storing incoming messages/comments from external channels with id, business_id, source (e.g., 'twitter', 'email'), author (who sent the message), content (text), received_at, etc. This will likely be implemented Post-MVP.
Database Schema - Naming Conventions: Go through the schema and ensure all names use "business" terminology:
If an existing template had organization_id in tables like content or events, rename those columns to business_id.
Ensure foreign key relationships are correctly set (e.g., content.business_id -> businesses.id, business_members.user_id -> auth.users.id etc.).
Update any functions or triggers that used old names (if any).
Enable Row Level Security (RLS): For multi-tenant security, turn on RLS for tables that will be directly queried from frontend (if using Supabase client on front):
Enable RLS on businesses, business_members, content, etc.
Write policies so that users can only see and manipulate data for businesses they belong to:
E.g., on content: "Members of a business can select content where content.business_id is one of the business IDs the user belongs to." Similar for insert: allow if the user is a member of that business (and perhaps check they match user_id on insert).
On businesses: allow a user to select or update their own business if they are a member; allow insert to create new business (maybe unrestricted insert with the rule that on insert, owner_id is set to their UID via a trigger or function).
On business_members: allow users to see their memberships, and maybe allow insert if an invite system (or allow owners to add members).
For MVP, you might keep it simple by performing access checks in backend code instead, but Supabase RLS is a robust approach if using Supabase directly from the frontend.
Supabase Storage (for images): Create a storage bucket (e.g., generated-images) in Supabase if the application will save images generated by DALL-E:
Configure public access or signed URLs depending on needs. Possibly mark it private and serve via signed URLs for security.
This will be used by the backend to upload images and by the frontend to retrieve them for display.
2. Supabase Auth & User Management
Auth Schema and Triggers: Supabase’s auth.users table holds basic user info (id, email). Set up a trigger or function to automatically create a profile or membership on sign-up:
For example, create a Postgres function that, after a new user registers, inserts a new row into profiles (with the user’s ID and perhaps default name as their email prefix) so that profile exists.
Also, you might insert a default membership if you want every user to have a personal business by default. However, better to let them create or join manually (so maybe not auto-creating a business).
If an invite system is in place and the user’s email matches an invite, you could auto-add them to that business on sign-up (this can be complex, could be Post-MVP).
Verify Email (Optional): Decide if new users need to verify their email before using the app:
For MVP, you might skip email verification for simplicity (Supabase allows turning it off).
If left on, ensure the front-end handles the case (the user might need to click a link in email; Supabase will mark them confirmed).
Role Management: In the business_members table, enforce through logic or additional columns that the user who creates a business is assigned the 'owner' role:
Possibly create a trigger on inserting into businesses to also insert a business_members entry linking the user (owner) to that business.
Alternatively, handle this in application code when calling the DB (create business then membership).
Ensure that there’s at least one owner per business and maybe prevent non-owners from certain actions (like adding other members, editing business settings). These checks can be done in backend code or RLS policies using the role field.
Integrate Supabase Auth in Node (Backend): The Node/Express backend may need to verify tokens from the frontend:
When the frontend makes API calls (e.g., generate content, publish content), it should include the Supabase Auth JWT (e.g., the access token). Configure Express middleware to parse and verify this token using Supabase’s JWKS or a library. Supabase JWTs are signed with the project’s secret.
Alternatively, the frontend could send the session or user ID and you manually check, but verifying JWT is more secure. Supabase provides the JWT secret in settings for server-side verification.
Once verified, you can trust the req.user or the JWT’s payload (which includes the user’s UID and possibly role claims) to identify the caller.
Testing Auth Flows: After setup, test the full signup -> login -> create business flow:
Create a test user via the front-end signup, ensure the profile and membership are created in the DB (adjust triggers if not working).
Try logging in, ensure the JWT is valid and the front-end shows the correct state.
Try creating a second user and inviting them (if invite flow done) or ensure they cannot see the first user’s data (RLS working).
This ensures the foundation is solid before moving to more complex features.
3. Business Logic & Multi-Tenancy Enforcement
Business Creation Backend: Implement an API endpoint in Express (e.g., POST /api/businesses) or allow direct Supabase call to create a new business:
If using direct Supabase from front-end, you might not need a specific endpoint; but if you want to enforce additional logic (like auto-creating membership via backend), implement the route.
In the route handler, extract the user ID from the auth token (ensuring the user is authenticated), then insert a new business record into the businesses table (using Supabase client or direct SQL).
Also insert into business_members to link the user as owner. If using Supabase client with the service key, you can bypass RLS or use the user’s token with RLS properly configured.
Return the new business ID or object to the frontend.
Business Join Backend: If join/invite is part of MVP, implement an endpoint (e.g., POST /api/businesses/join) that handles invitation codes:
Validate the invite code (look up in invites table if exists and not used).
If valid, insert a new business_members entry with the current user as a member of that business (role could be default 'member').
Mark the invite as used. Return success so front-end can proceed.
Handle error cases: invalid code or already member. Return appropriate status so front-end can show a message.
Data Access Control: Ensure throughout the backend code that every data query or mutation is scoped to the user's business:
For example, when a user requests content list, the backend (or RLS policy) should only fetch content where business_id is one the user has membership in. If using RLS and the user's JWT, this is automatic if policies are set. If using the service role, you must include checks in the query.
When inserting new content or events, ensure the business_id is taken from the user's context (active business) and not directly from client input (to avoid spoofing). If the front-end sends the business_id, double-check that user is a member.
Similarly, for any update/delete operations (though MVP might not have content editing or deletion, if it does, enforce business ownership).
Replace "Organization" with "Business": If any existing backend logic or names used "organization":
Rename any variables, database references, or file names to "business". For instance, if there was organizationId in some code, change it to businessId.
Update any user-facing messages or logs as well.
This task is largely find-and-replace but also ensure new code consistently uses the business terminology.
Testing Multi-Tenancy: Simulate multiple businesses with test data:
Create two businesses and two users, with one user belonging to both businesses (if applicable). Populate some content in each.
Test that when user is active in Business A, they cannot fetch or see content of Business B (try via API calls or direct DB queries with user token).
Verify that switching context (if implemented via front-end) properly loads the other business’s data and not the previous one’s.
This testing ensures the data separation is working (crucial for security in multi-tenant systems).
4. Node/Express API Server Setup
Initialize Express App: Set up a new Node.js project (if not done already). Install Express and necessary middleware (like body-parser for JSON). Create an index.js or server.js that starts an Express server listening on an appropriate port.
Consider using TypeScript for the backend for better structure (ts-node or compile step), since the project is new. If so, configure a tsconfig and use common Express typings.
Add basic middleware: express.json() for JSON body parsing, maybe cors to allow the frontend domain to call the APIs (especially if frontend is on a different origin during dev).
For now, allow all CORS in development; lock it down to the known domain in production.
Environment Config: Create a .env file for the backend (or use Fly.io secrets) to store:
Supabase Service Role key (for full DB access if needed).
Supabase Project URL.
Supabase JWT secret (if verifying tokens manually).
OpenAI API keys (for GPT-4 and DALL-E).
Redis URL or connection details for BullMQ.
Any API keys for external services (e.g., Twitter API keys for distribution).
Load these env vars at server start (using dotenv package) so they can be used in config.
Supabase Admin Client: Install Supabase Node client (@supabase/supabase-js) on the backend as well. Initialize it with the Service Role key (this gives the backend full privileges to read/write regardless of RLS, which is useful for server-side operations).
Alternatively, connect directly to the Postgres database using a library like pg or Prisma. But using Supabase client is straightforward.
If using Supabase client, be careful to respect RLS if you want (it can be bypassed by using service key). For many admin tasks (like inserting content, events), using service key is fine as long as you ensure to supply correct business_id to avoid cross-tenant issues.
Auth Middleware: Implement an Express middleware to protect routes:
Write a function to extract the Supabase JWT from the Authorization header (Bearer token) or a cookie (depending on how front-end passes it; front-end could just use supabase.auth.getSession() and send the access token).
Verify the JWT. Supabase tokens are JWTs signed with the project's secret. You can use jsonwebtoken library with the secret (available in Supabase settings) to verify and decode the token.
On valid token, attach the decoded token (user ID, etc.) to req.user or req.auth.
If token missing or invalid, respond with 401 for protected endpoints.
For simplicity, you could also skip manual JWT verification by requiring the front-end to call Supabase directly for data operations. But since we need to call OpenAI and such, verifying tokens is good to ensure the request is from a valid user.
Define API Routes: Set up Express route handlers for each needed endpoint:
POST /api/generate-content – to handle AI content generation requests.
POST /api/generate-image – (optional) handle image generation if done separately.
POST /api/distribute – to handle content publishing/scheduling.
GET /api/content – to fetch list of content (though front-end could use Supabase directly, but maybe needed if we want to filter on backend).
POST /api/businesses and /api/businesses/join – if managing business creation/join through backend instead of directly.
GET /api/analytics – if generating some analytics data server-side.
These endpoints should use the auth middleware to ensure the user is logged in.
Also, consider prefixing routes with /api or version like /v1 as needed.
Testing Express Server: Create a simple test route (e.g., /api/ping returning "pong") and try calling it from the frontend (temporarily) to ensure CORS and basic setup works. This helps catch any config issues early.
5. AI Text Generation Service (OpenAI GPT-4)
Install OpenAI SDK: Add the OpenAI client library (openai npm package) to the backend. Alternatively, use direct HTTPS requests if preferred. Using the SDK simplifies API calls.
API Key Configuration: Use the OpenAI API key from env to configure the client. Keep in mind using GPT-4 and DALL-E will require appropriate access (OpenAI API keys with GPT-4 enabled and image generation access).
Implement Generate Content Logic: In the /api/generate-content route handler, implement the logic to create AI-generated text:
Extract the prompt and parameters from the request body (e.g., { prompt: "...", contentType: "blog"/"tweet"/etc } if provided).
Construct a request to OpenAI. For GPT-4, use the Chat Completion API:
Model: use the latest GPT-4 model available (e.g., "gpt-4" or if a specialized model like gpt-4-0613).
Formulate the messages: you can use a system message to guide style (“You are an AI copywriter...”) and a user message with the prompt or instructions.
If the content type is provided (say user chose "Tweet"), include that in the system or user prompt to guide the response.
Call openai.createChatCompletion (if using SDK) or appropriate method and await the result.
Handle the response: extract the generated text content from response.data.choices[0].message.content.
Error handling: wrap the call in try/catch and return a 500 error with message if OpenAI API fails or times out. Possibly pass through specific error details for the front-end (like if it's a validation issue).
Streaming vs Synchronous: Decide if you will stream the response:
For MVP, simplest is to get the full completion then return it in the JSON response. (Streaming would allow partial results to front-end, but that complicates the front-end side; it can be a nice-to-have later.)
If not streaming, you might also implement a timeout if OpenAI calls are slow, to avoid hanging the Express request too long. But generally GPT-4 should respond in a reasonable time for short prompts.
Return Response: Send back a JSON response to the client with the generated content:
e.g., { success: true, content: "generated text...", ... }. If images are also part of it, you might include an image URL or reference (though image generation might be separate, see next section).
If you plan to immediately save this content to the database as a draft, you can do so here: insert into content table with published = false and return the ID. This way the content is stored. Alternatively, let the front-end decide to save or not.
Include any identifiers or additional data if needed (like usage tokens count if you track, or the prompt echo).
Testing GPT-4 Integration: Try calling the endpoint with a sample prompt via a REST client or from the running frontend:
Check that the OpenAI API responds and you get content back.
Test with various prompt lengths or types to see response format (ensure newlines, etc., are preserved as needed).
Verify error handling by e.g. giving an empty prompt or intentionally using an invalid key to see the error path.
Logging Usage/Telemetry: After successfully generating content, log an event in the events table (using Supabase client):
For example, record an event type "content_generated" with the business_id, user_id, maybe the length of content or content type. This will be useful for analytics (how many generations per day, etc).
This can be done asynchronously (don’t slow the response; e.g., fire and forget the insert, or use the job queue for logging if necessary, though probably overkill).
Also consider logging OpenAI usage if needed for cost tracking (the API returns tokens used in the response; you could store that in the event data).
6. AI Image Generation Service (OpenAI DALL-E 3)
Image Generation Endpoint: Provide an endpoint such as POST /api/generate-image or extend the content endpoint to handle image requests when needed:
If the front-end requests an image (maybe the user clicked an "Generate Image" button or included in prompt), handle accordingly. You could have a separate field in the request like includeImage: true or a separate route.
For clarity, a separate /api/generate-image that takes an image prompt might be easier to maintain.
Call OpenAI Image API: OpenAI’s latest image model (DALL·E 3, possibly via the openai API or via GPT-4 with image creation):
If using the DALL-E API: Use openai.createImage or similar method. Provide the prompt (which could be derived from the content or user input explicitly for the image). DALL-E API will return a URL to the generated image or a base64 string depending on parameters.
If using GPT-4 with image abilities (GPT-4 Vision): As of 2025, OpenAI might allow a unified API call to GPT-4 to get an image. If that's available (e.g., a function call or a different model endpoint like gpt-4-vision), you can use that. Otherwise, stick to DALL-E endpoint for now since that’s straightforward.
Request one image (for MVP) with desired quality. You can use size 1024x1024 or lower to save time/cost, depending on needs.
Save or Return Image: Once the image is generated:
Save to Supabase Storage: Using the Supabase storage bucket configured, upload the image file. You might get the image as a URL (likely a link to an OpenAI CDN). It’s better to fetch that URL and then upload the binary to your own storage if you want persistent control, because OpenAI’s link might expire after a short time.
Use an HTTP library to download the image from the URL (if OpenAI returns a URL) and then use Supabase storage API or AWS SDK if using S3, to store it (e.g., supabase.storage.from('generated-images').upload('business123/abc.png', fileBuffer, ...)).
Alternatively, OpenAI can return base64 (if you request it), which you can directly convert to a buffer and upload.
If saving, store the public URL or path in the content.image_url field or possibly in a separate table if needed. Supabase can generate a public URL if the bucket is public, or you can create signed URLs for the frontend.
Return in Response: Return the image URL or an identifier to the frontend. For example, { success: true, imageUrl: "https://.../generated-images/business123/abc.png" }.
If not saving server-side (maybe not needed if front-end will immediately use it), you could return the OpenAI URL directly. But it’s safer to store it to ensure the user can view it later.
Integrate with Content Flow: If the image generation is part of the content generation workflow:
You might call the image generation after getting text content, if the feature is like “generate an accompanying image”. Or allow the user to do it separately.
Make sure to associate the image with the correct content record (e.g., update the content entry with the image URL after generation).
This can be done by having the image endpoint accept a contentId to link to, or doing it all in one go in a combined endpoint.
Error Handling: Similar to text generation, handle errors gracefully:
If OpenAI fails to generate an image (it can happen if prompt violates guidelines or other issues), catch the error and return a 500 with a message.
The front-end should be informed to show an error (e.g., “Image generation failed. Try a different prompt.”).
Also log these errors for debugging (maybe log an event "image_gen_failed" with reason).
Test Image Generation: Manually call the image API with a test prompt using a tool or temporary route:
Ensure the image is generated and you can upload it to Supabase and retrieve it.
Test the time it takes; if it’s slow, consider making this an async job (the next section with queue could cover offloading this to the worker so the HTTP request doesn’t time out).
Check that the image URL returned is accessible from the frontend (if using private bucket, you might need to use Supabase’s auth token to fetch it, or switch to public bucket for ease in MVP).
7. Job Queue Setup (BullMQ & Redis)
Set Up Redis: Provision a Redis instance. If deploying on Fly.io, you can use Fly’s built-in Redis (if available) or a service like Upstash for a serverless Redis. For local dev, run a Redis docker or use a local installation.
Store the Redis connection URL in the backend .env. E.g., REDIS_URL=....
Install ioredis or let BullMQ handle it via configuration.
Install BullMQ: Add bullmq npm package to the backend. BullMQ will manage job queues and scheduling using Redis.
Initialize Queue(s): In your Node app, set up a BullMQ Queue instance for each type of background job:
For example, create a contentGenerationQueue for handling heavy AI generation jobs (if you decide to offload them) and a distributionQueue for scheduled publishing.
You might have something like:
import { Queue } from 'bullmq';
const redisConnection = { connection: { host: '...', port: ..., password: '...' } };
const contentQueue = new Queue('contentGenerationQueue', redisConnection);
const distQueue = new Queue('distributionQueue', redisConnection);
Use distinct names for the queues.
Queue Processor/Worker: Create a separate worker script or process to actually process jobs:
This could be an separate Node script (e.g., worker.js) that you run in parallel (on Fly.io you might run it as a separate VM or process).
In worker.js, use BullMQ’s Worker:
import { Worker } from 'bullmq';
const contentWorker = new Worker('contentGenerationQueue', async job => { ... }, redisConnection);
const distWorker = new Worker('distributionQueue', async job => { ... }, redisConnection);
Implement the processing logic inside: for content generation, the job data would include prompt, user, business, etc. The worker can call OpenAI (similar to what we did in the express route) and save content to DB. For distribution, the job data includes contentId and maybe channel info; the worker will post to the external service.
Ensure to handle job completion, errors (catch exceptions so the job fails gracefully). BullMQ can retry jobs if configured.
Using the Queue in Routes: Modify the Express routes to use the queue:
In the /api/generate-content route, instead of directly calling OpenAI, you could add a job to contentGenerationQueue:
Job data might include: prompt, content options, userId, businessId.
Optionally immediately respond to frontend with a job ID and a status like “queued”. However, that complicates frontend to poll for result.
For MVP, you might choose to synchronously handle generation in the request (simpler). Offloading to a queue is more about scaling and not freezing the web thread. It’s optional for the text generation if performance is okay. It might be more useful for distribution scheduling which is naturally async.
In the /api/distribute route, if a schedule time is provided or even for immediate, add a job to distributionQueue:
If schedule time is now or very soon, you can enqueue it to run immediately. If it’s future, BullMQ supports delay or schedule (with a third-party plugin or using job delay property).
Job data: contentId (so the worker can fetch the content text/image and destination), channel info (like 'twitter'), maybe the user’s auth token or keys (or fetch those from DB).
Respond to the frontend immediately that the content is scheduled or publishing. The actual posting will happen in the background.
Job Queue Configuration: Set any needed options:
Set concurrency for workers (maybe allow a few jobs at once if needed).
Set attempts for jobs if you want auto-retry on failure (e.g., retry posting a tweet once if network error).
Ensure the Redis connection is robust (BullMQ will reconnect on lost connection by default).
For distribution, if timing is crucial, ensure the server time is correct and consider time zones when scheduling (likely fine if using UTC everywhere).
Monitor Jobs (Optional): For debugging, you can log when jobs are completed or failed:
BullMQ allows hooks or events on job completion. You could log or even send a WebSocket message to frontend (Post-MVP idea) to notify the user that a job finished.
There are also admin UI tools (like Bull Board or Arena) you could integrate later to monitor the queue. Not needed for MVP but useful for dev debugging if issues arise.
8. Content Distribution Integration (e.g., Twitter API)
Select a Distribution Channel: For MVP, choose one channel to integrate, for example Twitter (X), since it’s common for sharing content:
Register a developer app on the platform to get API credentials (for Twitter, you need a Bearer token for OAuth 2.0 or use OAuth 1.1a for user context posting).
As an MVP shortcut, you might use your own account token to post on behalf of a single account (not ideal for multi-user, but easier). Proper implementation would involve OAuth flow for each user, which is complex (likely Post-MVP to allow arbitrary user accounts).
Store API Credentials: Add necessary API keys/secrets to the environment or database:
For Twitter with OAuth2, you might have a Bearer token that allows posting as the app (if using OAuth2 user context, you need a user access token and secret).
Since multi-user auth is hard for MVP, one approach: have a config table or just env variables for a single Twitter account’s token that will be used for all posts (again, not scalable to real multi-user product, but fine for demo).
Alternatively, integrate a simpler channel like a dummy email sender (Mailgun or SendGrid) to email the content to someone just to demo distribution.
Implement Posting Logic: In the distribution worker (BullMQ) or directly in the Express route if not queueing, implement the call to the external API:
For Twitter: Use an API library like twitter-api-v2 (npm) or just POST to Twitter’s endpoint. Provide the content text (ensure it’s within length limits for a tweet or thread logic if needed).
If an image URL is present and you want to attach the image: Twitter API requires uploading the image first to get a media_id. That’s additional steps (upload media, then attach media_id to tweet). Could skip image in tweet for MVP if too complex, or implement it if time allows.
On success, Twitter API will return the posted Tweet ID or URL. Capture that.
On failure, handle errors (e.g., rate limit or auth issues) and mark job as failed. The job can retry or you can log the failure for the user to be informed.
Update Content Status: After successful posting:
Update the corresponding content record in the DB: set published = true, published_at = now(), platform = 'twitter' (or whichever), and platform_post_id = <tweet_id> (store the tweet ID or a URL).
This way, the content is marked as distributed and you have a reference to fetch metrics later or display a link.
If the job is scheduled and runs later, this update will happen at that time.
Immediate Response Handling: For immediate publish (user clicks and we enqueue and post right away):
The front-end is not waiting for the job result in the current design. But it would be nice to inform the user of success or failure.
Post-MVP idea: use WebSockets or Supabase Realtime to send a notification. For MVP, a simpler approach: after a short delay, front-end could refetch the content list or status. Or when the user navigates to analytics next time, they’ll see it was published.
Possibly implement a quick polling: Frontend could call an endpoint like /api/content/{id} after a few seconds to see if published became true.
Extend to Other Channels (Post-MVP): Design the distribution logic in a way that adding new channels (LinkedIn, Facebook, Email) would be manageable:
Perhaps abstract the posting function by channel type (one function for Twitter, one for others) and call the appropriate one based on job data.
For now, keep it simple with one channel, but keep the code organized (e.g., in a services/twitterClient.js) so new integrations can plug in similarly later.
Test Distribution End-to-End: Do a test run:
Generate some sample content (or manually insert a content record).
Enqueue a distribution job (or call the route) to post it.
Verify the content appears on the target platform (e.g., check the Tweet was posted).
Check the database content record updated correctly with published info.
Test error scenario: use an invalid token or intentionally cause an error to ensure your error handling path works (the job should not crash the worker entirely).
This will confirm the distribution pipeline is functioning properly for the chosen channel.
9. Telemetry and Event Logging (Backend)
Implement Event Logging Helpers: Create a simple utility or service for logging events to the database. For example, a function logEvent(userId, businessId, type, data) that inserts into the events table:
Use the Supabase client (with service key) or direct SQL for insertion.
This could also be done via Supabase’s REST API, but since we have direct DB access, it’s straightforward to use the client.
Ensure to capture at least event_type, business_id, user_id, and a timestamp (can default to now()).
The data field (JSON) can store misc details (like prompt length for a generation event, or error details for a failure event, etc.), but keep it optional or use null if not needed.
Hook into Key Actions: Call logEvent in the backend at important points:
After a user signs up or logs in (though login events might be better captured on frontend when they successfully log in, because backend might not always see a login event unless all interactions require login).
After content is generated (already noted in the generation task).
After a content is published (log an event "content_published" with maybe platform info).
If using invites, when an invite is sent or accepted.
On errors or unusual events, possibly log as well (or to a separate error logging system).
These logs are primarily for internal analytics, but could also help in debugging user issues.
Aggregate or Analyze (Basic): Although deep analytics is post-MVP, you can create some quick queries or materialized views for basic counts:
e.g., count of content_generated per business per day, count of content_published, etc., to see usage. Not necessary to expose now, but easy to have for admin.
You could expose a simple /api/analytics endpoint that returns some aggregate stats from the events table for the current business (like number of content pieces created, number published, last activity time).
If you choose to, the frontend Analytics page could use this. If not, it stays as a placeholder.
Performance Monitoring: Consider logging performance metrics such as time taken for generation or posting:
For example, measure how long the OpenAI API call took and log it in the event data or a separate performance table. This can help identify if certain prompts are slow or if the queue is backed up.
Monitor any slow database queries or heavy functions – but likely not a big issue for MVP scale.
Use console logs or a logging library to keep track of server operations in development; for production, plan to use a more robust logging approach (like storing logs or using a service).
Ensure Privacy: Since telemetry can include potentially sensitive data (like prompts or content text in the event data), make sure to treat it carefully:
Perhaps do not log full prompt text or content in events after all (or anonymize it), to avoid storing too much personally or business-specific data in the telemetry. Summary or length might suffice.
This consideration might be minor for MVP, but good to keep in mind for trust (especially if the platform will have many users).
10. Basic Performance Analytics (User-Facing)
This is somewhat optional for MVP, but we include a minimal approach to show content performance. More advanced analytics will be a Post-MVP enhancement.
Collect Basic Metrics: For any content published to external platforms, gather any immediate metrics if available:
Some platforms return initial engagement data (most don’t give much instantly). For example, Twitter API can fetch tweet details including like count, but right after posting it will be zero anyway.
Perhaps record the number of followers at time of posting (if accessible) as a context metric (to gauge reach potential).
If using an email channel, you might record how many recipients (if you had that data).
For MVP, you might skip this due to complexity of external APIs. Instead, focus on tracking within-app events as done above.
Analytics Dashboard (simplistic): Implement an Analytics page or section in the frontend that shows a few key numbers using the data we have:
Total number of content pieces created by the current business.
Total published (or percentage of created that are published).
Maybe the most recent content performance: if you stored external IDs, you could have a button "Refresh metrics" that calls backend to fetch current stats for that post (Post-MVP likely).
If not pulling external data, the analytics might just be usage statistics (which is fine for an initial version): e.g., “You have generated 10 content pieces and published 4 of them. You have 2 team members.”
Use a simple query on events or content table to get these numbers and return via an /api/analytics call.
Implement /api/analytics (simple): Create an endpoint that gathers some stats for the current business:
It can do queries like: SELECT COUNT(*) FROM content WHERE business_id = X and similar for published count, etc., using Supabase client or direct SQL.
Also maybe get last 5 events from events table to show recent activities (e.g., “User A published Content Y on 2025-04-26”).
Return this as JSON. Keep the logic simple and efficient (these are small tables in MVP).
Display Analytics Data: On the frontend Analytics page, display the data returned:
Use cards or a simple list to show “Content Created: N”, “Content Published: M”, “Team Members: K” etc.
List recent events or key actions in a feed style if available (nice-to-have).
This gives business owners a quick idea of activity. For actual content performance (likes, views), explain that those detailed metrics will come in the future (unless easy now).
Test with Sample Data: If possible, simulate a scenario:
Create a few content items, mark some as published (manually or via actual posting).
Manually insert a couple of event records (or use the app normally to generate them).
Call the analytics endpoint or open the page to see that it correctly counts and displays the info.
This ensures that when a user goes to Analytics, they see accurate (if basic) information rather than an empty page.
Post-MVP Tasks (Future Enhancements)
The following features and improvements are planned for after the MVP release. They are flagged as Post-MVP to be tackled once the core product is functional.
A. Unified Inbox for Multi-Channel Communications (Post-MVP)
Integrate Incoming Feeds: Expand the platform to aggregate messages and comments from various channels (e.g., replies to tweets, comments on LinkedIn posts, emails from newsletters) into a single Unified Inbox for each business.
Set up webhooks or periodic fetch jobs for each integrated platform: e.g., Twitter webhook or poll for mentions/replies, email integration via IMAP or API, etc.
Create an inbox_messages table to store these communications with fields like business_id, source (channel), message_text, sender_name, timestamp, metadata (JSON) for any extra info (like message ID, thread ID).
Ensure each incoming message is tagged with the relevant content or context if possible (e.g., link a reply to the content piece it responded to via content_id or store the external post ID to match it).
Backend Services for Inbox: Develop separate background workers or services to handle incoming data:
For social media: use the platform’s APIs to retrieve messages. For instance, Twitter’s API can provide mentions or replies for a given tweet/user. Schedule a job (with BullMQ or a cron) to fetch new messages every few minutes, or set up webhooks if available.
For email: integrate with an email service or API (like SendGrid parse webhook or a dedicated email address per business to collect replies). When an email is received, forward it to the app via webhook and store it as an inbox message.
Use the Supabase messages/inbox table to insert these incoming messages. Consider using Supabase Realtime or a WebSocket to notify the frontend in real-time when a new message arrives (for live updates in the UI).
Inbox UI: Build the Inbox page on the frontend to display messages:
Show a list of conversations or messages grouped by source or content piece. For example, a list of recent interactions with filters for channel (All, Twitter, Email, etc.).
Allow the user to click on a message to view the full conversation thread. This may involve linking back to the source (e.g., provide a link to view the tweet thread on Twitter) or showing the context if available (like the original post text and then the replies).
Provide a way to respond directly: e.g., reply to a tweet from within the app or send an email response. This requires sending via the channel’s API (which can be implemented similar to distribution, but in reverse).
Use components for a chat-like interface for each conversation thread, if applicable.
Access Control: Ensure only members of the business see its messages and that sending responses uses the correct credentials (likely the ones used to post originally).
Testing and Refinement: This feature is complex; test gradually: first get read-only listing of messages working for one channel, then add more channels and then reply capability.
B. Advanced Performance Analytics (Post-MVP)
External Analytics Integration: Extend the analytics system to fetch and display performance metrics of published content across channels:
For each content piece that has platform and platform_post_id, integrate with the platform’s API to retrieve metrics. For example, Twitter’s API can provide tweet engagement (likes, retweets, impressions if using v2 endpoints with Elevated access). LinkedIn or others have their own analytics endpoints.
Set up scheduled jobs (maybe daily or a few hours after posting) to pull these metrics and store them in a new content_metrics table. Fields: content_id, metric_type (e.g., 'likes', 'impressions'), value, retrieved_at. Or store as JSON per content for simplicity (depending on query needs).
Collect metrics like: number of likes, shares/retweets, comments, impressions/views, click-throughs (if link present), etc., as available from each platform’s API.
Analytics Dashboard Enhancements: Revamp the Analytics page to show these performance metrics visually:
For each content piece, display its engagement (e.g., “Tweet about X: 10 likes, 2 replies, 1000 impressions”).
Charts or graphs: show trends over time, e.g., a line chart of content output per week, or a bar chart comparing engagement of different posts. Use a chart library (like Chart.js or Recharts) for a polished look.
If multiple channels are used, present aggregated metrics (like total audience reach across all platforms, total engagements, etc.).
Business Growth Metrics: Include metrics such as number of followers gained (if connecting social account data), or email open rates if using email. This might involve additional API calls (e.g., get Twitter follower count periodically).
Custom Reports and Export: Allow the user to export analytics data (CSV or PDF report) for their use. This might involve a back-end endpoint that compiles key stats over a period.
Optimization: As data grows, ensure queries are optimized (add indexes on content_metrics, etc.) and consider using Supabase PG functions or views to aggregate data for faster retrieval.
Data Privacy: Ensure compliance with any terms of service for pulling analytics data from platforms and present only to authorized users (the business owners).
C. Additional Distribution Channels (Post-MVP)
Social Platforms: After Twitter, integrate other popular platforms:
LinkedIn: Post content to LinkedIn company pages or user feeds. Requires OAuth flow for LinkedIn and using their API to share posts.
Facebook: Post to Facebook pages (needs Graph API integration and app approval for posting).
Instagram: If relevant for content (though IG is image-focused, maybe later if generating images, with the IG Graph API for business accounts).
Pinterest, Reddit, etc.: Depending on user needs, any platform with an API could be added.
For each new platform, create a similar integration: handle OAuth (store tokens per user/business), posting logic, and possibly incoming comments for Inbox and metrics for Analytics.
Email Newsletters: Integrate an email distribution mechanism:
Allow businesses to maintain an email subscriber list (create a subscribers table with emails tied to business).
Connect to an email service API (SendGrid, Mailgun, AWS SES) to send out generated content as an email newsletter or announcement.
Provide options in distribution UI to “Send as Email” and possibly a simple email template.
Track email opens/clicks via the email service webhook for analytics.
Content Scheduling UI Improvements: Develop a calendar view or timeline for scheduled posts:
Users could see a calendar with scheduled content (like a content calendar) and adjust timing by drag-and-drop (requires front-end work and updating BullMQ job delays).
Ensure editing or canceling scheduled posts is possible (BullMQ allows job removal by ID).
Platform Credentials Management: Create a Settings section for managing connected accounts:
List connected social accounts for the business, allow adding new ones (which triggers OAuth flows), and removing connections.
Securely store OAuth tokens (in Supabase, possibly encrypted) and refresh tokens as needed.
This way, distribution jobs use the correct user account tokens rather than a global one.
D. Enhanced User Management & Security (Post-MVP)
Social Login Options: Add OAuth login options (Google, GitHub, etc.) via Supabase Auth or a custom implementation to make it easier for users to sign up. Supabase can directly enable Google and others; integrating those would broaden login choices beyond email/password.
Role-Based Access Control: Expand roles beyond just 'owner' and 'member':
Define roles like Admin, Editor, Viewer with different permissions (e.g., only Admin can invite new users or access analytics, Editor can create content but not publish, etc.).
Implement checks in the UI and backend to enforce these (could utilize RLS policies with role column, and/or application logic).
Audit Logs: Maintain a more detailed audit trail for actions (especially important when multiple team members use the system): who edited settings, who deleted content, etc. The events table may suffice, but you might create a separate audit_logs if needed for clarity.
Improved Invite System: If not in MVP, implement full invite flow:
Ability for an owner to send an invite email via the app (use an email service to send an invite link).
The link directs to a frontend route that calls backend to verify the invite token and adds the user to the business (post sign-up or login).
Show pending invites in the business settings with ability to revoke.
Multi-Tenancy Hardening: If using RLS, continue to test and ensure no data leaks. If not using RLS (relying on backend checks), consider moving to RLS for stronger guarantees as the system scales and more direct access to DB might happen (like using Supabase from front-end).
Scalability & Deployment: For production readiness beyond MVP:
Set up proper Fly.io deployment workflows (maybe using Fly’s Postgres if not using Supabase for DB, but here we are). Possibly deploy the Next.js (if it were Next, but it’s Vite static) and Node backend on Fly with monitoring.
Introduce logging and error tracking services (e.g., Sentry for frontend and backend) to catch runtime errors as users start using the product.
Set up a staging environment to test features before production deploys.
E. UI/UX Enhancements (Post-MVP)
Polish UI with Tailwind/Shadcn: Refine the interface with better styling, animations, and states:
Use shadcn/ui components more extensively (toasts for notifications, modals for confirmations, skeletons for loading, etc.).
Ensure consistent theming (maybe add dark mode toggle if desired).
Make the interface more guided (tooltips or help text for new users, especially for the AI prompt input to suggest what to enter).
Tutorial/Onboarding Flow: Implement an onboarding tour for new users/businesses:
Example: after first login, highlight “Create Business” button, then guide to “Generate Content” with a sample prompt, etc.
This can improve user engagement and understanding of the product.
Feedback Mechanism: Provide a way for users to give feedback or report issues directly in the app (could be as simple as a mailto link or a form that sends to your email). This is not directly about development tasks, but a nice addition to improve the product iteratively.
Optimize Performance: As the app grows, revisit performance:
Use code-splitting in front-end for faster load (if the bundle gets large).
Add caching where applicable on backend (though small scale likely fine).
Optimize images or use CDN for static assets (Tailwind/JS files are small, but any images, etc.).
Testing & QA: Post-MVP, write more comprehensive tests:
Frontend component and integration tests for major flows (login, content generation).
Backend unit tests for utility functions (if any) and possibly integration tests hitting the test database.
This will ensure stability as new features are added.
With this structured plan, the team (and any assisting AI agents) can easily follow the steps to build out the MarketPeak MVP. The tasks are organized to first establish the core foundations (auth, businesses, UI), then implement the key AI content generation feature, and finally add distribution and basic analytics. The post-MVP section provides a clear roadmap for future enhancements like the unified inbox and advanced analytics. Following this plan will result in a functional MVP ready to deploy on Fly.io, and a clear path forward for subsequent iterations. Good luck with the development!